<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Honey, I Shrunk the Neural Net: Optimizing for the Edge | Ritesh Roshan</title>
    <link rel="stylesheet" href="/blog-post/style.css?v=2026-unified">
</head>

<body>
    <header>
        <div class="container">
            <a href="https://riteshroshann.github.io/blog-post/" class="logo">
                <div class="logo-icon"></div>
                Blog Post by rox
            </a>
            <nav>
                <a href="/blog-post/about/">About</a>
                <a href="/blog-post/">Blogs</a>
                <a href="https://github.com/riteshroshann" target="_blank" class="social-link">
                    <svg class="social-icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"
                        xmlns="http://www.w3.org/2000/svg">
                        <path
                            d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
                    </svg>
                    GitHub
                </a>
                <a href="https://www.linkedin.com/in/ritesh-roshan-sahoo/" target="_blank" class="social-link">
                    <svg class="social-icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"
                        xmlns="http://www.w3.org/2000/svg">
                        <path
                            d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z" />
                    </svg>
                    LinkedIn
                </a>
            </nav>
        </div>
    </header>

    <main class="article-wrapper">
        <div class="article-header">
            <div class="article-meta">
                January 11, 2026 &nbsp;|&nbsp; Ritesh Roshan Sahoo
            </div>
            <h1>Honey, I Shrunk the Neural Net: Optimizing for the Edge</h1>
        </div>

        <div class="container">
            <figure style="margin: 0; width: 100%;">
                <img src="https://images.unsplash.com/photo-1555255707-c07966088b7b?q=80&w=2000&auto=format&fit=crop"
                    alt="Microchip macro shot"
                    style="width: 100%; border-radius: 12px; height: auto;">
            </figure>
        </div>

        <article class="article-content">
            <p class="lead">
    We've seen this pattern before. ENIAC filled a room. Smartphones fit in a pocket. Then we built planet-scale data centers. Now we're shrinking again. Is this cycle fundamental to how technology evolves, or are we missing something deeper about the nature of computation itself?
</p>

<p>
    The AI scaling race feels eerily familiar to anyone who remembers the megahertz wars of the 1990s. Bigger, faster, more—until physics said no. Then we pivoted: multi-core, parallelism, specialization. <strong>Are we approaching a similar inflection point with neural networks?</strong> Or is this cycle itself a red herring—a distraction from the real question: <em>What is the fundamental unit of intelligence?</em>
</p>

<h2>1. The Tyranny of the Cycle: Big → Small → Big → ?</h2>
<p>
    In 1946, ENIAC weighed 30 tons. By 1981, the IBM PC sat on a desk. By 2010, AWS was building warehouse-scale computers. Now in 2026, we're trying to run LLMs on hearing aids.
</p>
<p>
    This isn't just history repeating—it's a <strong>dialectic driven by economics and physics</strong>. Centralization wins when:
</p>
<ul>
    <li><strong>Economies of scale dominate:</strong> Shared infrastructure amortizes fixed costs (cloud computing, model training).</li>
    <li><strong>Network effects matter:</strong> Value increases with interconnection (social platforms, federated learning).</li>
    <li><strong>Coordination is hard:</strong> Centralized control simplifies deployment and updates.</li>
</ul>
<p>
    Decentralization wins when:
</p>
<ul>
    <li><strong>Latency kills:</strong> Speed-of-light is non-negotiable (autonomous vehicles, AR/VR).</li>
    <li><strong>Privacy regulations tighten:</strong> GDPR, data sovereignty, medical records.</li>
    <li><strong>Energy costs explode:</strong> Moving data is more expensive than computing locally.</li>
</ul>
<p>
    But here's the twist: <strong>quantum computing broke the cycle</strong>. It didn't make classical computers smaller—it changed the game entirely. Are we on the cusp of a similar paradigm shift in AI? Neuromorphic chips? Biological computation? Or is compression just another turn of the same wheel?
</p>

<h2>2. The Lottery Ticket Hypothesis: Maybe We Never Needed to Go Big</h2>
<p>
    Jonathan Frankle's 2019 bombshell: dense networks contain sparse subnetworks—"winning tickets"—that train to comparable accuracy when initialized correctly. <strong>This suggests the entire scaling era might have been inefficient search</strong>.
</p>
<p>
    Think about it: If a ResNet-50 can be pruned to 10% density post-training, why did we need the other 90% at all? The answer is uncomfortable: <em>We don't know how to find the sparse solution directly</em>. Overparameterization is a crutch for bad optimization.
</p>
<p>
    This is reminiscent of the <strong>No Free Lunch theorem</strong>: no single algorithm dominates across all problems. We scaled up because gradient descent in high dimensions is easy. But the lottery ticket hypothesis hints that there's a more elegant path—we just haven't found it yet.
</p>
<blockquote>
    "The initialization matters more than the weights. The topology matters more than the scale."
</blockquote>
<p>
    But here's the rub: <strong>Transformers resist pruning</strong> better than CNNs resist quantization. The all-to-all attention mechanism seems fundamentally dense. This isn't a bug—it's a hint. Maybe <em>some kinds of intelligence require density</em>, while others benefit from sparsity. Vision: sparse. Language: dense. Planning: ?
</p>

<h2>3. Quantization: Dancing at the Boundary of Chaos</h2>
<p>
    The fact that neural networks tolerate 4-bit quantization is <em>astonishing</em>. We're reducing 32 bits of floating-point precision down to 16 discrete values. That's throwing away 99.95% of the representational space. And yet—perplexity barely budges.
</p>
<p>
    Why does this work? Two competing theories:
</p>
<ul>
    <li><strong>The Lottery Ticket View:</strong> Most weights are noise. Quantization is just aggressive regularization that forces the model to use its "real" parameters.</li>
    <li><strong>The Loss Landscape View:</strong> The optimal solution sits in a wide, flat basin. Small perturbations (quantization noise) don't significantly change the output because the gradient is nearly zero.</li>
</ul>
<p>
    Both can't be fully true. If weights are noise, why do models benefit from scale? If the landscape is flat, why does fine-tuning improve performance? The answer is likely <strong>layer-dependent</strong>: early layers (feature extractors) are robust and compressible. Late layers (task heads) are brittle and precision-sensitive.
</p>
<p>
    But we're hitting a wall at ~3 bits. Below that, catastrophic degradation. <strong>Is this fundamental?</strong> Information theory says no—if the model truly learns a compressed representation, we should be able to quantize arbitrarily. The fact that we can't suggests our models are <em>not yet optimal representations</em>. They're approximations that happen to work.
</p>
<figure>
    <img src="https://images.unsplash.com/photo-1635070041078-e363dbe005cb?q=80&w=2000&auto=format&fit=crop" 
        alt="Digital neural network visualization"
        style="width: 100%; border-radius: 12px; height: auto;">
    <figcaption style="text-align: center; font-size: 0.9rem; color: #666; margin-top: 0.5rem;">
        The phase transition from analog to digital intelligence.
    </figcaption>
</figure>

<h2>4. The Quantum Computer Analogy: Are We Asking the Wrong Question?</h2>
<p>
    When quantum computers arrived, the question wasn't "how do we make classical computers smaller?" It was "what problems have quantum structure?" Shor's algorithm for factoring. Grover's for search. Quantum simulation for chemistry. <strong>The paradigm shift wasn't scale—it was substrate</strong>.
</p>
<p>
    Are we making the same mistake with AI? We're trying to compress Transformers—but maybe Transformers are the wrong architecture for edge deployment. CNNs dominated vision not because they're "better," but because they match the structure of images: local, hierarchical, translation-invariant.
</p>
<p>
    What's the "quantum" equivalent for intelligence? Candidates:
</p>
<ul>
    <li><strong>Neuromorphic computing (e.g., Intel Loihi, IBM TrueNorth):</strong> Spiking neural networks that operate asynchronously, event-driven, like biological neurons. Orders of magnitude more efficient—but require rethinking everything about training.</li>
    <li><strong>In-memory computing:</strong> Analog matrix multiplication using resistive crossbars. Eliminates the von Neumann bottleneck. But noisy, imprecise, and hard to manufacture at scale.</li>
    <li><strong>Photonic neural networks:</strong> Computation at the speed of light, literally. But training is an open problem—backprop assumes differentiable digital operations.</li>
</ul>
<p>
    The pattern: <strong>efficiency requires co-designing algorithms and hardware</strong>. Transformers were designed for GPUs. If we want edge intelligence, we need architectures designed for edge constraints: sparsity, locality, asynchrony.
</p>

<h2>5. The Thermodynamic Limit: 20 Watts</h2>
<p>
    The ultimate benchmark isn't other silicon. It's biology. The human brain runs on ~20 watts. It does few-shot learning, complex planning, and motor control with an energy budget that wouldn't power a lightbulb.
</p>
<p>
    Current AI is orders of magnitude away from this <strong>Landauer limit</strong>. Training GPT-4 cost an estimated 50 gigawatt-hours. We are bruteforcing intelligence with energy.
</p>
<p>
    This discrepancy suggests that our current paradigm—dense matrix multiplication on high-frequency clock cycles—is wildly inefficient foundationally. The future might not be "smaller" digital networks, but <strong>hybrid analog-digital systems</strong> where we embrace noise rather than fight it. Using the physics of the device to perform the computation (like a memristor) rather than simulating boolean logic gates.
</p>

<h2>Conclusion: The End of the Monolith</h2>
<p>
    We aren't just shrinking models. We are witnessing the <strong>speciation of intelligence</strong>.
</p>
<p>
    The era of the "General Purpose Monolith" will be look back upon like the era of the Mainframe. Impressive, important, but ultimately a transition state. The future is a swarm of highly specialized, ultra-efficient, locally-running agents that coordinate loosely.
</p>
<p>
    We are moving from building <em>Gods in the Cloud</em> to breeding <em>Insects in the Dust</em>. And as any evolutionary biologist will tell you: the insects run the world.
</p>

<hr>

<h3>References & Further Reading</h3>
<p class="references" style="font-size: 0.85rem; color: #666; line-height: 1.6;">
    [1] Frankle, J., & Carbin, M. (2019). <em>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.</em><br>
    [2] Hestness, J., et al. (2017). <em>Deep Learning Scaling is Predictable, Empirically.</em> (The "Scaling Laws" paper)<br>
    [3] Hooker, S. (2020). <em>The Hardware Lottery.</em> (Why we use the algorithms that fit our chips, not the ones that are best)<br>
    [4] LeCun, Y. (2022). <em>A Path Towards Autonomous Machine Intelligence.</em> (On the limits of LLMs)<br>
    [5] Mead, C. (1990). <em>Neuromorphic Electronic Systems.</em>
</p>
</article>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <div class="footer-logo">
                        <div class="footer-logo-icon"></div> Blog Post by rox
                    </div>
                    <p class="footer-tagline">
                        A digital garden where research meets reverie. This is the synthesis of everything I am
                        learning—unfiltered and constantly evolving.
                    </p>
                </div>

                <div class="footer-links-group">
                    <div class="footer-column">
                        <h4>Explore</h4>
                        <a href="/blog-post/about/">About</a>
                        <a href="/blog-post/">Blogs</a>
                    </div>
                    <div class="footer-column">
                        <h4>Legal</h4>
                        <a href="/blog-post/privacy-policy/">Privacy Policy</a>
                        <a href="/blog-post/terms/">Terms of Service</a>
                    </div>
                </div>
            </div>

            <div class="footer-bottom">
                <div class="copyright">
                    © 2026 Blog Post by rox. All rights reserved.
                </div>
                <div class="footer-socials">
                    <a href="https://github.com/riteshroshann" target="_blank" title="GitHub">
                        <svg class="footer-social-icon" viewBox="0 0 24 24">
                            <path
                                d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
                        </svg>
                    </a>
                    <a href="https://x.com/RiteshRoshann" target="_blank" title="X (Twitter)">
                        <svg class="footer-social-icon" viewBox="0 0 24 24">
                            <path
                                d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z" />
                        </svg>
                    </a>
                    <a href="https://www.linkedin.com/in/ritesh-roshan-sahoo/" target="_blank" title="LinkedIn">
                        <svg class="footer-social-icon" viewBox="0 0 24 24">
                            <path
                                d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z" />
                        </svg>
                    </a>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>
